{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5X1ZNIia7_3",
        "outputId": "d5c43a81-0a1c-41d8-ef7f-72c0cf6a147c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is already installed. Version: 2.18.0\n",
            "pyDOE is already installed.\n",
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "##############################################################################################################\n",
        "# RUN THIS CODE BLOCK FIRST, it will load dependencies / install them if your system does not already have them.\n",
        "##############################################################################################################\n",
        "\n",
        "# tensorflow\n",
        "try:\n",
        "    # Try to import TensorFlow\n",
        "    import tensorflow as tf\n",
        "    print(f\"TensorFlow is already installed. Version: {tf.__version__}\")\n",
        "except ImportError as e:\n",
        "    # TensorFlow is not installed; install it\n",
        "    print(\"TensorFlow is not installed. Installing TensorFlow...\")\n",
        "    !pip install tensorflow\n",
        "    import tensorflow as tf\n",
        "    print(f\"Successfully installed TensorFlow. Version: {tf.__version__}\")\n",
        "\n",
        "# pyDOE\n",
        "try:\n",
        "    # Try to import pyDOE\n",
        "    import pyDOE\n",
        "    print(f\"pyDOE is already installed.\")\n",
        "except ImportError as e:\n",
        "    # pyDOE is not installed; install it\n",
        "    print(\"pyDOE is not installed. Installing pyDOE...\")\n",
        "    !pip install pyDOE\n",
        "    import pyDOE\n",
        "    print(f\"Successfully installed pyDOE.\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "@author: Yongji Wang, Yao Lai, Ray Chou (modified from Maziar Raissi)\n",
        "\"\"\"\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR-528eT8Mj-"
      },
      "source": [
        "# Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Rby8iLPdbD9Z"
      },
      "outputs": [],
      "source": [
        "class PhysicsInformedNN:\n",
        "    # Initialize the class with training and validation data, network structure, and parameters\n",
        "    def __init__(self, x_u, u, x_f, x_val, layers, lb, ub, gamma):\n",
        "        # Lower and upper bounds of the input domain (used for normalization)\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        # Supervised training data: locations and known values\n",
        "        self.x_u = x_u  # Collocation points with known solution\n",
        "        self.u = u      # Known solution values at x_u\n",
        "\n",
        "        # Collocation points for enforcing the physics (residual of ODE)\n",
        "        self.x_f = x_f\n",
        "\n",
        "        # Validation points (used for monitoring generalization)\n",
        "        self.x_val = x_val\n",
        "\n",
        "        # Neural network architecture: list of neurons per layer\n",
        "        self.layers = layers\n",
        "\n",
        "        # Weight for the ODE loss (residual) in total loss\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # Initialize network weights and biases\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "\n",
        "        # List of all trainable variables (used by optimizer)\n",
        "        self.train_variables = self.weights + self.biases\n",
        "\n",
        "        # # Compute initial loss (not strictly necessary here, could be deferred)\n",
        "        # self.loss = self.loss_NN()\n",
        "\n",
        "    '''\n",
        "    Neural Network Initialization\n",
        "    =============================\n",
        "    '''\n",
        "\n",
        "    # Initialize the weights and biases using Xavier initialization\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    # Xavier initialization helps maintain signal variance across layers\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "\n",
        "    def neural_net(self, X, weights, biases):\n",
        "        \"\"\"\n",
        "        Function to construct the forward pass of the neural network\n",
        "        Input:\n",
        "        1. X: input tensor\n",
        "        2. weights: All W matrixes, NN parameters\n",
        "        3. biases: All b matrixes, NN parameters\n",
        "        Output:\n",
        "        1. Y: NN prediction\n",
        "        \"\"\"\n",
        "        num_layers = len(weights) + 1\n",
        "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0 #normalize the input to [-1, 1]\n",
        "        #########################\n",
        "        #########################\n",
        "        ### YOUR CODE STARTS HERE (~8 lines, 5 points)\n",
        "        ### Note: we do not apply activation function for the last layer\n",
        "\n",
        "\n",
        "        ### YOUR CODE ENDS HERE\n",
        "        #########################\n",
        "        #########################\n",
        "\n",
        "        return Y\n",
        "\n",
        "    '''\n",
        "    Physics-Informed Loss Construction\n",
        "    ==================================\n",
        "    '''\n",
        "\n",
        "    # Predict the solution u(x) using the neural network\n",
        "    def net_u(self, x):\n",
        "        u = self.neural_net(x, self.weights, self.biases)\n",
        "        return u\n",
        "\n",
        "    #\n",
        "    def net_f(self, x):\n",
        "        \"\"\"\n",
        "        Function to Compute the ODE residual f = du/dx - u using automatic differentiation\n",
        "        Output:\n",
        "        1. f: equation residual\n",
        "        \"\"\"\n",
        "\n",
        "        ### More details of tf.GradientTape() you can follow\n",
        "        ### https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x)\n",
        "            u = self.net_u(x)\n",
        "            #########################\n",
        "            #########################\n",
        "            ### YOUR CODE STARTS HERE (~2 lines, 5 points)\n",
        "\n",
        "\n",
        "\n",
        "        ### YOUR CODE ENDS HERE\n",
        "        #########################\n",
        "        #########################\n",
        "        return f\n",
        "\n",
        "    @tf.function\n",
        "    def loss_NN(self):\n",
        "        \"\"\"\n",
        "        Function to compute the total loss: data loss + equation loss (weighted by gamma)\n",
        "        Output:\n",
        "        1. loss_d: data loss\n",
        "        2. loss_e: equation loss\n",
        "        3. loss: total loss\n",
        "        \"\"\"\n",
        "        #########################\n",
        "        #########################\n",
        "        ### YOUR CODE STARTS HERE (~5 lines, 5 points)\n",
        "        ### Hint: Use mean squared loss for both equation loss and data loss\n",
        "\n",
        "\n",
        "        ### YOUR CODE ENDS HERE\n",
        "        #########################\n",
        "        #########################\n",
        "        loss = loss_d + self.gamma * loss_e\n",
        "        return loss, loss_d, loss_e\n",
        "\n",
        "    # Train the model using Adam optimizer\n",
        "    def train(self, nIter: int, learning_rate: float):\n",
        "        \"\"\"\n",
        "        Function used for training the model using the Adam optimizer.\n",
        "        Implements exponential decay for learning rate.\n",
        "        Tracks training and validation loss at intervals.\n",
        "        \"\"\"\n",
        "\n",
        "        # Learning rate schedule: exponential decay\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=learning_rate,\n",
        "            decay_steps=400,\n",
        "            decay_rate=0.8\n",
        "        )\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        start_time = time.time()\n",
        "        it_list = []          # Track training iterations\n",
        "        loss_list = []        # Track total loss\n",
        "        eq_loss_list = []     # Track equation loss\n",
        "        val_it_list = []      # Validation iteration steps\n",
        "        val_loss_list = []    # Validation loss values\n",
        "\n",
        "        for it in range(nIter):\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, loss_d, loss_e = self.loss_NN()\n",
        "                grads = tape.gradient(loss, self.train_variables)\n",
        "                optimizer.apply_gradients(zip(grads, self.train_variables))\n",
        "\n",
        "                it_list.append(it)\n",
        "                loss_list.append(loss)\n",
        "                eq_loss_list.append(loss_e)\n",
        "\n",
        "            # Every 10 steps, evaluate validation loss and print status\n",
        "            if (it + 1) % 10 == 0:\n",
        "                residual = self.net_f(self.x_val)\n",
        "                val_loss = tf.reduce_mean(tf.square(residual)) * self.gamma\n",
        "                val_it_list.append(it)\n",
        "                val_loss_list.append(val_loss)\n",
        "\n",
        "                elapsed = time.time() - start_time\n",
        "                print('It: %d, Train Loss: %.4e, Data Loss: %.4e, Eq Loss: %.4e, Val Loss: %.4e Time: %.2f' %\n",
        "                      (it, loss.numpy(), loss_d.numpy(), loss_e.numpy(), val_loss.numpy(), elapsed))\n",
        "                start_time = time.time()\n",
        "\n",
        "        return it_list, loss_list, eq_loss_list, val_it_list, val_loss_list\n",
        "\n",
        "    # Predict u and residual f for new input points\n",
        "    @tf.function\n",
        "    def predict(self, x):\n",
        "        u_p = self.net_u(x)\n",
        "        f_p = self.net_f(x)\n",
        "        return u_p, f_p\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bStG1G1-8xHL"
      },
      "source": [
        "# NN initialization and Training!\n",
        "This might take a few minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3lKoYNJT8xiQ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1243)\n",
        "'''\n",
        "Define the hyper-parameter\n",
        "============================================\n",
        "'''\n",
        "\n",
        "# # FOR Q2.1 and 2.2, please use these parameters (start)\n",
        "layers = [1, 20, 1]\n",
        "learning_rate = 0.5\n",
        "niter = 1000\n",
        "gamma = 1\n",
        "# # FOR Q2.1 and 2.2, please use these parameters (end)\n",
        "\n",
        "\n",
        "# FOR Q2.3, please use these parameters (start)\n",
        "# layers = [1, 20, 20, 1]\n",
        "# learning_rate = 0.04\n",
        "# niter = 6000\n",
        "# gamma = 1\n",
        "# FOR Q2.3, please use these parameters (end)\n",
        "\n",
        "\n",
        "'''\n",
        "Preparing the data for training / validation\n",
        "============================================\n",
        "'''\n",
        "\n",
        "### Please do not change anything (Start)\n",
        "N_validation = 401\n",
        "x_validation = np.linspace(0,2,N_validation)[:,None]\n",
        "u_validation = np.exp(x_validation)\n",
        "# Doman bounds\n",
        "lb = x_validation.min()\n",
        "ub = x_validation.max()\n",
        "data_type = tf.float32\n",
        "### Please do not change anything (End)\n",
        "\n",
        "\n",
        "### For Q2.1 and 2.2 (Start)\n",
        "N_collocation = 10 # We start with 10 collocation points. You might want to change number of collocation points to answer the questions\n",
        "x_train = tf.constant([0.], dtype=data_type)[:,None]\n",
        "u_train = tf.constant([1.], dtype=data_type)[:,None]\n",
        "x_f_train = lb + (ub-lb)*lhs(1, N_collocation)\n",
        "x_f_train = np.vstack((x_f_train, x_train))\n",
        "# Note, above line, we add the datapoint into the collocation point set, not neccessary,\n",
        "# feel free to change it if you feel like\n",
        "\n",
        "x_f_train = tf.cast(x_f_train, dtype=data_type) #transform the numpy array x_f_train into a tensor\n",
        "### For Q2.1 and 2.2 (End)\n",
        "\n",
        "\n",
        "'''\n",
        "Initialize the NN (you are not required to change the code here)\n",
        "============================================\n",
        "'''\n",
        "model = PhysicsInformedNN(x_u=x_train, u=u_train, x_f=x_f_train,\n",
        "                          x_val=tf.constant(x_validation, dtype=data_type)[:,None],\n",
        "                          layers=layers, lb=lb, ub=ub, gamma=gamma)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "ax.scatter(x_validation, np.exp(x_validation), s=1, label='validation points')\n",
        "ax.scatter(x_f_train, np.exp(x_f_train), s=20, color='red', marker='*', label='collocation points')\n",
        "ax.scatter(x_train, u_train, s=20, color='black', marker='*', label='data points')\n",
        "ax.set_ylabel('U')\n",
        "ax.set_xlabel('x')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zYma0ltwmnu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Start training\n",
        "start_time = time.time()\n",
        "it_list, loss_list, eq_loss_list, val_it_list, val_loss_list = model.train(niter, learning_rate)\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "u_pred_val, f_pred_val = model.predict(tf.cast(x_validation, dtype=data_type))"
      ],
      "metadata": {
        "id": "p3FVoTmpmmwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "######################################################################\n",
        "############################# Plotting ###############################\n",
        "######################################################################\n",
        "\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "ax.plot(it_list, loss_list, label='training loss (total)')\n",
        "ax.plot(it_list, eq_loss_list, label='training loss (eq)')\n",
        "ax.plot(val_it_list, val_loss_list, label='validation loss (eq)')\n",
        "ax.set_title('Training Loss Curve')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('iteration')\n",
        "ax.set_ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kPKj6o00F_9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = [10, 10], dpi = 300)\n",
        "\n",
        "ax = plt.subplot(211)\n",
        "ax.plot(x_validation, u_validation, 'b-', linewidth = 2, label = 'exact (validation points)')\n",
        "ax.plot(x_validation, u_pred_val, 'r--', linewidth = 2, label = 'predict (validation points)')\n",
        "ax.scatter(x_f_train, np.exp(x_f_train), marker='*', s=50, label = 'collocation points')\n",
        "ax.scatter(x_train, u_train, marker='*', s=50, label = 'data points')\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 12)\n",
        "ax.set_ylabel('$u$', fontsize = 12, rotation = 0)\n",
        "ax.set_title('solution', fontsize = 15)\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "ax = plt.subplot(212)\n",
        "ax.plot(x_validation, f_pred_val, 'b-', linewidth = 2)\n",
        "ax.scatter(x_f_train, np.zeros(len(x_f_train)), marker='*', s=50, label = 'collocation points', alpha=0.8)\n",
        "ax.scatter(x_train, np.zeros(len(x_train)), marker='*', s=50, label = 'data points', alpha=0.8)\n",
        "\n",
        "ax.set_xlabel('$x$', fontsize = 12)\n",
        "ax.set_ylabel('$f$', fontsize = 12, rotation = 0)\n",
        "ax.set_title('equation residual', fontsize = 15)"
      ],
      "metadata": {
        "id": "bJjiAloPpd2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CotgHEY_8NK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
