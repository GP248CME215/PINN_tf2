{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpj4ffuDytoj"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GP248CME215/GP248CME215/PINN_tf2/edit/main/Burgers_equation_25_spring_TF2.ipynb)\n",
        "\n",
        "\n",
        "\n"
      ]
   },
  {
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5X1ZNIia7_3",
        "outputId": "e4b44db3-c9b7-4b34-df1a-1c2be82ff9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is already installed. Version: 2.18.0\n",
            "pyDOE is already installed.\n",
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "##############################################################################################################\n",
        "# RUN THIS CODE BLOCK FIRST, it will load dependencies / install them if your system does not already have them.\n",
        "##############################################################################################################\n",
        "\n",
        "# tensorflow\n",
        "try:\n",
        "    # Try to import TensorFlow\n",
        "    import tensorflow as tf\n",
        "    print(f\"TensorFlow is already installed. Version: {tf.__version__}\")\n",
        "except ImportError as e:\n",
        "    # TensorFlow is not installed; install it\n",
        "    print(\"TensorFlow is not installed. Installing TensorFlow...\")\n",
        "    !pip install tensorflow\n",
        "    import tensorflow as tf\n",
        "    print(f\"Successfully installed TensorFlow. Version: {tf.__version__}\")\n",
        "\n",
        "# pyDOE\n",
        "try:\n",
        "    # Try to import pyDOE\n",
        "    import pyDOE\n",
        "    print(f\"pyDOE is already installed.\")\n",
        "except ImportError as e:\n",
        "    # pyDOE is not installed; install it\n",
        "    print(\"pyDOE is not installed. Installing pyDOE...\")\n",
        "    !pip install pyDOE\n",
        "    import pyDOE\n",
        "    print(f\"Successfully installed pyDOE.\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "@author: Yongji Wang, Yao Lai, Ray Chou (modified from Maziar Raissi)\n",
        "\"\"\"\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook accompanies the CME 215 Spring 2025 Problem Set on **Physics-Informed Neural Networks (PINNs)**. The focus is on solving the **1D viscous Burgers’ equation**, a fundamental partial differential equation (PDE) in fluid dynamics that exhibits nonlinear convection and diffusion behavior. It is frequently used as a benchmark problem due to its rich mathematical structure and availability of an analytical solution.\n",
        "\n",
        "We aim to solve the following form of the Burgers’ equation:\n",
        "\n",
        "$\\frac{\\partial u}{\\partial t} = \\lambda_1 u \\frac{\\partial u}{\\partial x} - \\lambda_2 \\frac{\\partial^2 u}{\\partial x^2}, \\quad x \\in [-1, 1], \\quad t \\in [0, 1]$,\n",
        "\n",
        "where \\( u(x,t) \\) is the unknown velocity field, and $\\lambda_1$, $\\lambda_2$ are scalar parameters corresponding to advection and diffusion, respectively.\n",
        "\n",
        "The initial and boundary conditions are given by:\n",
        "\n",
        "$u(0, x) = -\\sin(\\pi x), \\quad u(t, -1) = u(t, 1) = 0$.\n",
        "\n",
        "In this notebook, we use a PINN model that incorporates both data and physical constraints into a unified loss function:\n",
        "- **Data loss**: enforces agreement between the network prediction and known solution values at certain points\n",
        "- **Physics loss**: enforces that the network prediction satisfies the PDE at collocation points in the domain\n",
        "\n",
        "The model architecture uses a fully connected feedforward neural network with:\n",
        "- 3 hidden layers, each with 20 neurons\n",
        "- Activation functions: `tanh` and `sin` (compared experimentally)\n",
        "- A single output node representing $u(x,t)$\n",
        "\n",
        "We train the model using the Adam optimizer and evaluate it against the ground truth solution loaded from `burgers_shock.mat`.\n"
      ],
      "metadata": {
        "id": "b0OOMTOAnFmL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR-528eT8Mj-"
      },
      "source": [
        "# Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rby8iLPdbD9Z"
      },
      "outputs": [],
      "source": [
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, X_u, u, X_f, layers, lb, ub, nu):\n",
        "        # Domain bounds for input normalization\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        # Split supervised data inputs (with known solution u) into x and t\n",
        "        self.x_u = X_u[:, 0:1]\n",
        "        self.t_u = X_u[:, 1:2]\n",
        "\n",
        "        # Split collocation points (where PDE is enforced) into x and t\n",
        "        self.x_f = X_f[:, 0:1]\n",
        "        self.t_f = X_f[:, 1:2]\n",
        "\n",
        "        # Known solution at supervised points\n",
        "        self.u = u\n",
        "\n",
        "        # Network architecture and PDE parameter\n",
        "        self.layers = layers\n",
        "        self.nu = nu\n",
        "\n",
        "        # Initialize neural network weights and biases\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "\n",
        "        # All trainable parameters for optimization\n",
        "        self.train_variables = self.weights + self.biases\n",
        "        # These are tf.Variable objects so updates here affect the model directly\n",
        "\n",
        "        # Initialize loss (this is not necessary here but allows pre-evaluation)\n",
        "        self.loss = self.loss_NN()\n",
        "\n",
        "    '''\n",
        "    Neural Network Initialization Functions\n",
        "    =======================================\n",
        "    '''\n",
        "\n",
        "    def initialize_NN(self, layers):\n",
        "        # Initialize weights and biases for each layer using Xavier init\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
        "            b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "    def xavier_init(self, size):\n",
        "        # Xavier/Glorot initialization to avoid vanishing/exploding gradients\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "    def neural_net(self, X, weights, biases):\n",
        "        # Forward pass through the fully connected feedforward neural network\n",
        "        num_layers = len(weights) + 1\n",
        "\n",
        "        # Normalize inputs to [-1, 1] using domain bounds\n",
        "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0\n",
        "\n",
        "        # Hidden layers with tanh activation\n",
        "        for l in range(0, num_layers - 2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "\n",
        "        # Output layer (linear activation)\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "\n",
        "    '''\n",
        "    Physics-Informed PDE Modeling\n",
        "    =============================\n",
        "    '''\n",
        "\n",
        "    def net_u(self, x, t):\n",
        "        # Predict u(x, t) by feeding concatenated input into the network\n",
        "        u = self.neural_net(tf.concat([x, t], 1), self.weights, self.biases)\n",
        "        return u\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "        # Construct the PDE residual f(x, t) = u_t + u*u_x - nu*u_xx using autograd\n",
        "        X = tf.concat([x, t], axis=1)\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch([x, t])\n",
        "\n",
        "            # Forward pass: predict u(x,t)\n",
        "            u = self.net_u(x, t)\n",
        "\n",
        "            # Compute gradients: first-order\n",
        "            u_x = tape.gradient(u, x)\n",
        "            u_t = tape.gradient(u, t)\n",
        "\n",
        "            # Compute second-order gradient: u_xx\n",
        "            u_xx = tape.gradient(u_x, x)\n",
        "\n",
        "        # Burgers' equation residual\n",
        "        f = u_t + u * u_x - self.nu * u_xx\n",
        "        return f\n",
        "\n",
        "    @tf.function\n",
        "    def loss_NN(self):\n",
        "        # Compute the total loss: data loss + PDE residual loss\n",
        "\n",
        "        # Prediction of u at supervised data points\n",
        "        self.u_pred = self.net_u(self.x_u, self.t_u)\n",
        "\n",
        "        # Residual of PDE at collocation points\n",
        "        self.f_pred = self.net_f(self.x_f, self.t_f)\n",
        "\n",
        "        # Mean squared error between predicted and known u\n",
        "        loss_data = tf.reduce_mean(tf.square(self.u - self.u_pred))\n",
        "\n",
        "        # Mean squared error of PDE residual\n",
        "        loss_eq = tf.reduce_mean(tf.square(self.f_pred))\n",
        "\n",
        "        # Total loss = data loss + physics loss (unweighted here)\n",
        "        return loss_data + loss_eq\n",
        "\n",
        "    def train(self, nIter: int, learning_rate: float):\n",
        "        \"\"\"\n",
        "        Train the model using Adam optimizer and gradient clipping.\n",
        "        Uses exponential decay learning rate and logs progress every 100 iterations.\n",
        "        \"\"\"\n",
        "        # Exponential learning rate decay, to help release the issue of training loss oscilation\n",
        "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "            initial_learning_rate=learning_rate,\n",
        "            decay_steps=200,\n",
        "            decay_rate=0.9\n",
        "        )\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "        varlist = self.weights + self.biases  # All trainable variables\n",
        "\n",
        "        # Logging variables\n",
        "        start_time = time.time()\n",
        "        it_list = []\n",
        "        loss_list = []\n",
        "\n",
        "        for it in range(nIter):\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss = self.loss_NN()\n",
        "\n",
        "            # Compute gradients of loss w.r.t. weights and biases\n",
        "            grads = tape.gradient(loss, varlist)\n",
        "\n",
        "            # Optional: clip gradients to prevent exploding updates\n",
        "            clipped_grads = [tf.clip_by_value(g, -1.0, 1.0) for g in grads]\n",
        "\n",
        "            # Apply gradients to update model parameters\n",
        "            optimizer.apply_gradients(zip(clipped_grads, varlist))\n",
        "\n",
        "            # Logging\n",
        "            it_list.append(it)\n",
        "            loss_list.append(loss)\n",
        "\n",
        "            if (it + 1) % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print('It: %d, Train Loss: %.3e, Time: %.2f' % (it, loss.numpy(), elapsed))\n",
        "                start_time = time.time()\n",
        "\n",
        "        return it_list, loss_list\n",
        "\n",
        "    @tf.function\n",
        "    def predict(self, X_star):\n",
        "        # Evaluate the trained model: predict both u and f at new points\n",
        "        u_star = self.net_u(X_star[:, 0:1], X_star[:, 1:2])\n",
        "        f_star = self.net_f(X_star[:, 0:1], X_star[:, 1:2])\n",
        "        return u_star, f_star\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAFMZoG1bOme"
      },
      "outputs": [],
      "source": [
        "# access the data on GitHub:\n",
        "!wget -O burgers_shock.mat https://raw.githubusercontent.com/GP248CME215/PINN_tf2/main/burgers_shock.mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMHAsv9u5VTx"
      },
      "source": [
        "# Data Initialization\n",
        "We load \\texttt{burgers\\_shock.mat}, which contains the numerical solution to the one-dimensional Burgers' equation. The dataset provides a matrix \\texttt{usol} of shape $256 \\times 100$, representing the solution $u(x, t)$ evaluated over a spatial grid $x$ and temporal grid $t$. Specifically, the spatial domain is defined over $-1 \\leq x \\leq 1$, and the temporal domain spans $0 \\leq t \\leq 1$. The range of the solution itself is also bounded within $-1 \\leq u \\leq 1$.\n",
        "\n",
        "It is important to ensure that the input variables $x$, $t$, and $u$ are on a relatively small scale when feeding them into a neural network. Smaller input values help improve the stability of training and prevent issues such as exploding or vanishing gradients, especially when using activation functions like $\\tanh$ or sigmoid. They also help ensure that the weights and biases of the model evolve smoothly during optimization, making the learning process more efficient.\n",
        "\n",
        "In this dataset, the inputs are already normalized within the range $[-1, 1]$, so no additional preprocessing is required before using them as input to the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmz5lOg7gI9a"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = scipy.io.loadmat('burgers_shock.mat')\n",
        "\n",
        "# Extract variables\n",
        "t = data['t'].squeeze()     # shape (100,)\n",
        "x = data['x'].squeeze()     # shape (256,)\n",
        "Exact = np.real(data['usol']).T        # shape (256, 100)\n",
        "\n",
        "print(f\"x shape: {x.shape}, t shape: {t.shape}, usol shape: {Exact.shape}\")\n",
        "# Note,\n",
        "\n",
        "# Plot the 2D field u(x, t)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.imshow(Exact.T, interpolation='nearest', cmap='rainbow',\n",
        "           extent=[t.min(), t.max(), x.min(), x.max()],\n",
        "           origin='lower', aspect='auto')\n",
        "plt.xlabel('Time t')\n",
        "plt.ylabel('x')\n",
        "plt.title(\"Solution to 1D Burgers' Equation\")\n",
        "plt.colorbar(label='u(x,t)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qPvmK0vebexV"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "nu = 0.01/np.pi\n",
        "# noise = 0.0\n",
        "\n",
        "N_u = 100  #\n",
        "N_f = 10000   # n\n",
        "\n",
        "X, T = np.meshgrid(x,t) #  X, T dimensions: 100 x 256\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  # dimension of 25600 x 2\n",
        "u_star = Exact.flatten()[:,None]                                # dimension of 25600 x 1\n",
        "\n",
        "# Domain bounds\n",
        "# Doman bounds\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))   #x,t coordinates of the training data at t=0\n",
        "uu1 = Exact[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1]))       #x,t coordinates of the training data at x=1\n",
        "uu2 = Exact[:,0:1]\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:]))       #x,t coordinates of the training data at x=-1\n",
        "uu3 = Exact[:,-1:]\n",
        "X_u_train = np.vstack([xx1, xx2, xx3])      #x,t coordinates of all training data at t=0, x=1, x=-1\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f)        #x,t coordinates of all collocation points\n",
        "# LHS: Latin Hypercube Sampling, https://pythonhosted.org/pyDOE/randomized.html\n",
        "# This youtube video may help you with understanding what Latin Hypercube Sampling is\n",
        "# https://www.youtube.com/watch?v=ugy7XC-cMb0\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "X_u_train = X_u_train[idx, :]\n",
        "u_train = u_train[idx,:]\n",
        "\n",
        "\n",
        "\n",
        "X_u_train = tf.cast(X_u_train, dtype=tf.float32)\n",
        "u_train = tf.cast(u_train, dtype=tf.float32)\n",
        "X_f_train = tf.cast(X_f_train, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bStG1G1-8xHL"
      },
      "source": [
        "# NN initialization and Training!\n",
        "This might take a few minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lKoYNJT8xiQ"
      },
      "outputs": [],
      "source": [
        "layers = [2, 20, 20, 20, 1]\n",
        "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, nu)\n",
        "\n",
        "start_time = time.time()\n",
        "it_list, loss_list = model.train(5000, learning_rate=5e-2)\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))\n",
        "\n",
        "### Note, here we only require you to train 5k iterations, which is sufficient for this P-set.\n",
        "### If you don't get exactly what Figure 1 (Pset2 pdf) shows, it is totally fine as it takes more iterations and time to train,\n",
        "### and we are not asking you to replicate the figure exactly. But you are supposed to get something\n",
        "### close to it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Training Evaluation"
      ],
      "metadata": {
        "id": "ev7_vwwAn_Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Make prediction on the mesh of the entire field\n",
        "X_star = tf.cast(X_star, dtype=tf.float32)\n",
        "u_pred, f_pred = model.predict(X_star)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "print('Error u: %e' % (error_u))\n",
        "\n",
        "U_pred = griddata(X_star, u_pred.numpy().flatten(), (X, T), method='cubic')\n",
        "Error = np.abs(Exact - U_pred)"
      ],
      "metadata": {
        "id": "RQgAFIReOIRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "######################################################################\n",
        "############################# Plotting ###############################\n",
        "######################################################################\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "ax.plot(it_list, loss_list)\n",
        "ax.set_title('Training Loss Curve')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('iteration')\n",
        "ax.set_ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "### Note it is common that you observe oscilations of loss curve along the training proceeds, this is due to\n",
        "### Adam optimizer. To eliminate the oscilations you can try learning rate schedulers, i.e.\n",
        "### tf.keras.optimizers.schedules.ExponentialDecay introduced in the training function (no guaranteed, osicilations\n",
        "### can still be seen here). Or you can try L-BFGS optimizer (not implemented in this code)\n",
        "### see https://www.tensorflow.org/probability/api_docs/python/tfp/optimizer/lbfgs_minimize"
      ],
      "metadata": {
        "id": "kPKj6o00F_9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khPMs9kQbo33"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = [10, 10], dpi = 300)\n",
        "\n",
        "####### Row 0: u(t,x) ##################\n",
        "gs0 = gridspec.GridSpec(1, 2)\n",
        "gs0.update(top=1-0.06, bottom=1-1/2, left=0.15, right=0.85, wspace=0)\n",
        "ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "\n",
        "\n",
        "ax.set_xlabel('$t$', fontsize = 15)\n",
        "ax.set_ylabel('$x$', fontsize = 15, rotation = 0)\n",
        "ax.set_title('$u(t,x)$', fontsize = 15)\n",
        "ax.legend(frameon=False, loc = 'best')\n",
        "\n",
        "####### Row 1: u(t,x) slices ##################\n",
        "gs1 = gridspec.GridSpec(1, 3)\n",
        "gs1.update(top=1-1/2, bottom=0.1, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 0])\n",
        "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "ax = plt.subplot(gs1[0, 1])\n",
        "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 2])\n",
        "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "ax.set_title('$t = 0.75$', fontsize = 10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
