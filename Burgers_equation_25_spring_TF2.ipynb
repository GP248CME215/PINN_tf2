{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5X1ZNIia7_3",
        "outputId": "b17048e5-e271-4ffb-bbed-fb6559039f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is already installed. Version: 2.18.0\n",
            "pyDOE is already installed.\n",
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "##############################################################################################################\n",
        "# RUN THIS CODE BLOCK FIRST, it will load dependencies / install them if your system does not already have them.\n",
        "##############################################################################################################\n",
        "\n",
        "# tensorflow\n",
        "try:\n",
        "    # Try to import TensorFlow\n",
        "    import tensorflow as tf\n",
        "    print(f\"TensorFlow is already installed. Version: {tf.__version__}\")\n",
        "except ImportError as e:\n",
        "    # TensorFlow is not installed; install it\n",
        "    print(\"TensorFlow is not installed. Installing TensorFlow...\")\n",
        "    !pip install tensorflow\n",
        "    import tensorflow as tf\n",
        "    print(f\"Successfully installed TensorFlow. Version: {tf.__version__}\")\n",
        "\n",
        "# pyDOE\n",
        "try:\n",
        "    # Try to import pyDOE\n",
        "    import pyDOE\n",
        "    print(f\"pyDOE is already installed.\")\n",
        "except ImportError as e:\n",
        "    # pyDOE is not installed; install it\n",
        "    print(\"pyDOE is not installed. Installing pyDOE...\")\n",
        "    !pip install pyDOE\n",
        "    import pyDOE\n",
        "    print(f\"Successfully installed pyDOE.\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "@author: Yongji Wang, Yao Lai, Ray Chou (modified from Maziar Raissi)\n",
        "\"\"\"\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from pyDOE import lhs\n",
        "import time\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "np.random.seed(123)\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR-528eT8Mj-"
      },
      "source": [
        "# Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Rby8iLPdbD9Z"
      },
      "outputs": [],
      "source": [
        "class PhysicsInformedNN:\n",
        "    # Initialize the class\n",
        "    def __init__(self, X_u, u, X_f, layers, lb, ub, nu):\n",
        "\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        self.x_u = X_u[:,0:1]\n",
        "        self.t_u = X_u[:,1:2]\n",
        "\n",
        "        self.x_f = X_f[:,0:1]\n",
        "        self.t_f = X_f[:,1:2]\n",
        "\n",
        "        self.u = u\n",
        "\n",
        "        self.layers = layers\n",
        "        self.nu = nu\n",
        "\n",
        "        # Initialize NNs\n",
        "        self.weights, self.biases = self.initialize_NN(layers)\n",
        "\n",
        "        # Create a list including all training variables\n",
        "        self.train_variables = self.weights + self.biases\n",
        "        # Key point: anything updates in train_variables will be\n",
        "        #            automatically updated in the original tf.Variable\n",
        "\n",
        "        # define the loss function\n",
        "        self.loss = self.loss_NN()\n",
        "\n",
        "\n",
        "    '''\n",
        "    Functions used to establish the initial neural network\n",
        "    ===============================================================\n",
        "    '''\n",
        "\n",
        "    def initialize_NN(self, layers):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0,num_layers-1):\n",
        "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
        "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
        "            weights.append(W)\n",
        "            biases.append(b)\n",
        "        return weights, biases\n",
        "\n",
        "\n",
        "\n",
        "    def xavier_init(self, size):\n",
        "        in_dim = size[0]\n",
        "        out_dim = size[1]\n",
        "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
        "        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "    def neural_net(self, X, weights, biases):\n",
        "        num_layers = len(weights) + 1\n",
        "\n",
        "        H = 2.0*(X - self.lb)/(self.ub - self.lb) - 1.0\n",
        "        for l in range(0,num_layers-2):\n",
        "            W = weights[l]\n",
        "            b = biases[l]\n",
        "            H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "        W = weights[-1]\n",
        "        b = biases[-1]\n",
        "        Y = tf.add(tf.matmul(H, W), b)\n",
        "        return Y\n",
        "\n",
        "\n",
        "    '''\n",
        "    Functions used to building the physics-informed contrainst and loss\n",
        "    ===============================================================\n",
        "    '''\n",
        "\n",
        "    def net_u(self, x, t):\n",
        "        u = self.neural_net(tf.concat([x,t],1), self.weights, self.biases)\n",
        "        return u\n",
        "\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "      # Combine x and t into a single input tensor for the NN\n",
        "      X = tf.concat([x, t], axis=1)\n",
        "\n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "          tape.watch([x, t])\n",
        "\n",
        "          # Forward pass through the neural network\n",
        "          u = self.net_u(x, t)\n",
        "\n",
        "          # First derivatives\n",
        "          u_x = tape.gradient(u, x)\n",
        "          u_t = tape.gradient(u, t)\n",
        "\n",
        "          # Second derivative\n",
        "          u_xx = tape.gradient(u_x, x)\n",
        "\n",
        "      # Burgers' equation residual: u_t + u * u_x - nu * u_xx = 0\n",
        "      f = u_t + u * u_x - self.nu * u_xx\n",
        "      return f\n",
        "\n",
        "    @tf.function\n",
        "    # calculate the physics-informed loss function\n",
        "    def loss_NN(self):\n",
        "        self.u_pred = self.net_u(self.x_u, self.t_u)\n",
        "        self.f_pred = self.net_f(self.x_f, self.t_f)\n",
        "        loss = tf.reduce_mean(tf.square(self.u - self.u_pred)) + \\\n",
        "               tf.reduce_mean(tf.square(self.f_pred))\n",
        "        return loss\n",
        "\n",
        "    def train(self, nIter: int, learning_rate: float):\n",
        "      \"\"\"\n",
        "      Function used for training the model\n",
        "      Corrections made on April 19th, 2025, replacing\n",
        "      optimizer.minimize(self.loss_NN, varlist)\n",
        "      with tf.tape()\n",
        "      the original setting no longer support by tf 2+\n",
        "      \"\"\"\n",
        "\n",
        "      lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate,\n",
        "                                                                   decay_steps=200,\n",
        "                                                                   decay_rate=0.9)\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "      # optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "      varlist = self.weights + self.biases  # all trainable parameters\n",
        "\n",
        "      start_time = time.time()\n",
        "      it_list = []\n",
        "      loss_list = []\n",
        "\n",
        "      for it in range(nIter):\n",
        "          with tf.GradientTape() as tape:\n",
        "              loss = self.loss_NN()\n",
        "          grads = tape.gradient(loss, varlist)\n",
        "          clipped_grads = [tf.clip_by_value(g, -1.0, 1.0) for g in grads]\n",
        "          optimizer.apply_gradients(zip(clipped_grads, varlist))\n",
        "          it_list.append(it)\n",
        "          loss_list.append(loss)\n",
        "          # Print progress\n",
        "          if (it+1) % 100 == 0:\n",
        "              elapsed = time.time() - start_time\n",
        "              print('It: %d, Train Loss: %.3e, Time: %.2f' % (it, loss.numpy(), elapsed))\n",
        "              start_time = time.time()\n",
        "\n",
        "      return it_list, loss_list\n",
        "\n",
        "    @tf.function\n",
        "    def predict(self, X_star):\n",
        "        u_star = self.net_u(X_star[:,0:1], X_star[:,1:2])\n",
        "        f_star = self.net_f(X_star[:,0:1], X_star[:,1:2])\n",
        "        return u_star, f_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAFMZoG1bOme"
      },
      "outputs": [],
      "source": [
        "# access the data on GitHub:\n",
        "!wget -O burgers_shock.mat https://raw.githubusercontent.com/GP248CME215/PINN_tf2/main/burgers_shock.mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMHAsv9u5VTx"
      },
      "source": [
        "# Data Initialization\n",
        "We load \\texttt{burgers\\_shock.mat}, which contains the numerical solution to the one-dimensional Burgers' equation. The dataset provides a matrix \\texttt{usol} of shape $256 \\times 100$, representing the solution $u(x, t)$ evaluated over a spatial grid $x$ and temporal grid $t$. Specifically, the spatial domain is defined over $-1 \\leq x \\leq 1$, and the temporal domain spans $0 \\leq t \\leq 1$. The range of the solution itself is also bounded within $-1 \\leq u \\leq 1$.\n",
        "\n",
        "It is important to ensure that the input variables $x$, $t$, and $u$ are on a relatively small scale when feeding them into a neural network. Smaller input values help improve the stability of training and prevent issues such as exploding or vanishing gradients, especially when using activation functions like $\\tanh$ or sigmoid. They also help ensure that the weights and biases of the model evolve smoothly during optimization, making the learning process more efficient.\n",
        "\n",
        "In this dataset, the inputs are already normalized within the range $[-1, 1]$, so no additional preprocessing is required before using them as input to the neural network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmz5lOg7gI9a"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "data = scipy.io.loadmat('burgers_shock.mat')\n",
        "\n",
        "# Extract variables\n",
        "t = data['t'].squeeze()     # shape (100,)\n",
        "x = data['x'].squeeze()     # shape (256,)\n",
        "Exact = np.real(data['usol']).T        # shape (256, 100)\n",
        "\n",
        "print(f\"x shape: {x.shape}, t shape: {t.shape}, usol shape: {Exact.shape}\")\n",
        "# Note,\n",
        "\n",
        "# Plot the 2D field u(x, t)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.imshow(Exact.T, interpolation='nearest', cmap='rainbow',\n",
        "           extent=[t.min(), t.max(), x.min(), x.max()],\n",
        "           origin='lower', aspect='auto')\n",
        "plt.xlabel('Time t')\n",
        "plt.ylabel('x')\n",
        "plt.title(\"Solution to 1D Burgers' Equation\")\n",
        "plt.colorbar(label='u(x,t)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "qPvmK0vebexV"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "nu = 0.01/np.pi\n",
        "# noise = 0.0\n",
        "\n",
        "N_u = 100  #\n",
        "N_f = 10000   # n\n",
        "\n",
        "X, T = np.meshgrid(x,t) #  X, T dimensions: 100 x 256\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))  # dimension of 25600 x 2\n",
        "u_star = Exact.flatten()[:,None]                                # dimension of 25600 x 1\n",
        "\n",
        "# Domain bounds\n",
        "# Doman bounds\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))   #x,t coordinates of the training data at t=0\n",
        "uu1 = Exact[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1]))       #x,t coordinates of the training data at x=1\n",
        "uu2 = Exact[:,0:1]\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:]))       #x,t coordinates of the training data at x=-1\n",
        "uu3 = Exact[:,-1:]\n",
        "X_u_train = np.vstack([xx1, xx2, xx3])      #x,t coordinates of all training data at t=0, x=1, x=-1\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f)        #x,t coordinates of all collocation points\n",
        "# LHS: Latin Hypercube Sampling, https://pythonhosted.org/pyDOE/randomized.html\n",
        "# This youtube video may help you with understanding what Latin Hypercube Sampling is\n",
        "# https://www.youtube.com/watch?v=ugy7XC-cMb0\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "X_u_train = X_u_train[idx, :]\n",
        "u_train = u_train[idx,:]\n",
        "\n",
        "\n",
        "\n",
        "X_u_train = tf.cast(X_u_train, dtype=tf.float32)\n",
        "u_train = tf.cast(u_train, dtype=tf.float32)\n",
        "X_f_train = tf.cast(X_f_train, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bStG1G1-8xHL"
      },
      "source": [
        "# NN initialization and Training!\n",
        "This might take a few minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lKoYNJT8xiQ"
      },
      "outputs": [],
      "source": [
        "layers = [2, 20, 20, 20, 1]\n",
        "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, layers, lb, ub, nu)\n",
        "\n",
        "start_time = time.time()\n",
        "it_list, loss_list = model.train(5000, learning_rate=5e-2)\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.4f' % (elapsed))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Make prediction on the mesh of the entire field\n",
        "X_star = tf.cast(X_star, dtype=tf.float32)\n",
        "u_pred, f_pred = model.predict(X_star)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "print('Error u: %e' % (error_u))\n",
        "\n",
        "U_pred = griddata(X_star, u_pred.numpy().flatten(), (X, T), method='cubic')\n",
        "Error = np.abs(Exact - U_pred)"
      ],
      "metadata": {
        "id": "RQgAFIReOIRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "######################################################################\n",
        "############################# Plotting ###############################\n",
        "######################################################################\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "ax.plot(it_list, loss_list)\n",
        "ax.set_title('Training Loss Curve')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('iteration')\n",
        "ax.set_ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kPKj6o00F_9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khPMs9kQbo33"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize = [10, 10], dpi = 300)\n",
        "\n",
        "####### Row 0: u(t,x) ##################\n",
        "gs0 = gridspec.GridSpec(1, 2)\n",
        "gs0.update(top=1-0.06, bottom=1-1/2, left=0.15, right=0.85, wspace=0)\n",
        "ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow',\n",
        "              extent=[t.min(), t.max(), x.min(), x.max()],\n",
        "              origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "fig.colorbar(h, cax=cax)\n",
        "\n",
        "ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "\n",
        "\n",
        "ax.set_xlabel('$t$', fontsize = 15)\n",
        "ax.set_ylabel('$x$', fontsize = 15, rotation = 0)\n",
        "ax.set_title('$u(t,x)$', fontsize = 15)\n",
        "ax.legend(frameon=False, loc = 'best')\n",
        "\n",
        "####### Row 1: u(t,x) slices ##################\n",
        "gs1 = gridspec.GridSpec(1, 3)\n",
        "gs1.update(top=1-1/2, bottom=0.1, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 0])\n",
        "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "ax = plt.subplot(gs1[0, 1])\n",
        "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "ax = plt.subplot(gs1[0, 2])\n",
        "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$u(t,x)$')\n",
        "ax.axis('square')\n",
        "ax.set_xlim([-1.1,1.1])\n",
        "ax.set_ylim([-1.1,1.1])\n",
        "ax.set_title('$t = 0.75$', fontsize = 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ZJzN37ae8L-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
